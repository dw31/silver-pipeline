{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver C - Delta Live Tables\n",
    "\n",
    "This notebook implements the Silver C stage using Delta Live Tables (DLT).\n",
    "Silver C performs deduplication and filtering on Silver B tables, representing the final Silver layer stage.\n",
    "\n",
    "## Key Features:\n",
    "- Ingests Silver B delta tables\n",
    "- Performs deduplication based on business key columns\n",
    "- Applies filtering rules and data quality scoring\n",
    "- Creates materialized Silver C tables for each source/lob/domain combination\n",
    "- Uses DLT decorators for automatic table creation and dependency management\n",
    "- Currently acts as pass-through but designed for future enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import dlt\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.window import Window\n",
    "from typing import Dict, List, Any, Optional\n",
    "import logging\n",
    "\n",
    "# Import pipeline modules\n",
    "from utils.config_loader import config_loader\n",
    "import silver_pipeline_stages as stages\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load configuration\n",
    "config = config_loader.load_config()\n",
    "source_combinations = config_loader.get_source_combinations()\n",
    "\n",
    "# Get active Spark session\n",
    "spark = SparkSession.getActiveSession()\n",
    "\n",
    "print(f\"Silver C DLT Pipeline - Configuration loaded\")\n",
    "print(f\"Processing {len(source_combinations)} source combinations\")\n",
    "print(f\"Silver schema: {config.silver_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication and Filtering Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for deduplication and filtering logic\n",
    "# These can be made configurable through business rules tables in the future\n",
    "\n",
    "DEDUPLICATION_CONFIG = {\n",
    "    # Domain-specific key columns for deduplication\n",
    "    \"pharmacy\": [\"patient_id\", \"ndc\", \"service_date\"],\n",
    "    \"medical\": [\"patient_id\", \"procedure_code\", \"service_date\"],  \n",
    "    \"member\": [\"member_id\", \"effective_date\"]\n",
    "}\n",
    "\n",
    "# Default key columns if domain-specific not found\n",
    "DEFAULT_DEDUP_KEYS = [\"patient_id\", \"service_date\"]\n",
    "\n",
    "# Quality score thresholds (for future enhancement)\n",
    "QUALITY_THRESHOLDS = {\n",
    "    \"min_quality_score\": 0.7,\n",
    "    \"completeness_threshold\": 0.8\n",
    "}\n",
    "\n",
    "print(f\"Deduplication configuration loaded:\")\n",
    "for domain, keys in DEDUPLICATION_CONFIG.items():\n",
    "    print(f\"  {domain}: {keys}\")\n",
    "print(f\"Default deduplication keys: {DEFAULT_DEDUP_KEYS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Silver C Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deduplication_keys(domain: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get deduplication key columns for a specific domain\n",
    "    \n",
    "    Args:\n",
    "        domain: Domain (pharmacy, medical, member)\n",
    "        \n",
    "    Returns:\n",
    "        List of column names to use for deduplication\n",
    "    \"\"\"\n",
    "    return DEDUPLICATION_CONFIG.get(domain, DEFAULT_DEDUP_KEYS)\n",
    "\n",
    "\n",
    "def apply_deduplication(df: DataFrame, dedup_keys: List[str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply deduplication logic to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        dedup_keys: List of columns to use for deduplication\n",
    "        \n",
    "    Returns:\n",
    "        Deduplicated DataFrame\n",
    "    \"\"\"\n",
    "    if not dedup_keys:\n",
    "        logger.info(\"No deduplication keys provided, returning original DataFrame\")\n",
    "        return df\n",
    "    \n",
    "    # Check which dedup keys exist in the DataFrame\n",
    "    available_keys = [key for key in dedup_keys if key in df.columns]\n",
    "    \n",
    "    if not available_keys:\n",
    "        logger.warning(f\"None of the deduplication keys {dedup_keys} found in DataFrame columns: {df.columns}\")\n",
    "        return df\n",
    "    \n",
    "    logger.info(f\"Applying deduplication using keys: {available_keys}\")\n",
    "    \n",
    "    # Add row number for deduplication (keep first occurrence)\n",
    "    window_spec = Window.partitionBy(*available_keys).orderBy(F.lit(1))\n",
    "    \n",
    "    deduplicated_df = (\n",
    "        df.withColumn(\"_row_num\", F.row_number().over(window_spec))\n",
    "          .filter(F.col(\"_row_num\") == 1)\n",
    "          .drop(\"_row_num\")\n",
    "    )\n",
    "    \n",
    "    original_count = df.count()\n",
    "    deduplicated_count = deduplicated_df.count()\n",
    "    duplicates_removed = original_count - deduplicated_count\n",
    "    \n",
    "    logger.info(f\"Deduplication complete: {original_count} -> {deduplicated_count} rows (removed {duplicates_removed} duplicates)\")\n",
    "    \n",
    "    return deduplicated_df\n",
    "\n",
    "\n",
    "def add_quality_score(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Add data quality score to DataFrame (placeholder for future enhancement)\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quality score column added\n",
    "    \"\"\"\n",
    "    logger.info(\"Adding quality score column (placeholder implementation)\")\n",
    "    \n",
    "    # Placeholder implementation - calculate completeness score\n",
    "    # In production, this would be more sophisticated\n",
    "    total_columns = len(df.columns)\n",
    "    \n",
    "    # Count non-null values per row and calculate completeness ratio\n",
    "    non_null_count = sum([F.when(F.col(col).isNotNull(), 1).otherwise(0) for col in df.columns])\n",
    "    \n",
    "    quality_df = df.withColumn(\n",
    "        \"quality_score\",\n",
    "        (non_null_count / total_columns).cast(\"decimal(3,2)\")\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Quality score column added based on field completeness\")\n",
    "    return quality_df\n",
    "\n",
    "\n",
    "def apply_quality_filtering(df: DataFrame, min_quality_score: float = 0.7) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply quality-based filtering (placeholder for future enhancement)\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame with quality_score column\n",
    "        min_quality_score: Minimum quality score threshold\n",
    "        \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    if \"quality_score\" not in df.columns:\n",
    "        logger.warning(\"No quality_score column found, skipping quality filtering\")\n",
    "        return df\n",
    "    \n",
    "    logger.info(f\"Applying quality filtering with minimum score: {min_quality_score}\")\n",
    "    \n",
    "    original_count = df.count()\n",
    "    filtered_df = df.filter(F.col(\"quality_score\") >= min_quality_score)\n",
    "    filtered_count = filtered_df.count()\n",
    "    \n",
    "    filtered_out = original_count - filtered_count\n",
    "    logger.info(f\"Quality filtering complete: {original_count} -> {filtered_count} rows (filtered out {filtered_out} low-quality rows)\")\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Silver C Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_c_table(source: str, lob: str, domain: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Create Silver C table for a specific source/lob/domain combination\n",
    "    \n",
    "    Args:\n",
    "        source: Source identifier\n",
    "        lob: Line of business\n",
    "        domain: Domain (pharmacy, medical, member)\n",
    "        \n",
    "    Returns:\n",
    "        Processed Silver C DataFrame\n",
    "    \"\"\"\n",
    "    logger.info(f\"Creating Silver C table for {source}/{lob}/{domain}\")\n",
    "    \n",
    "    # Read Silver B table using DLT\n",
    "    silver_b_table_name = f\"{source}_{lob}_{domain}_silver_b\"\n",
    "    \n",
    "    try:\n",
    "        # Use DLT read to get the Silver B table\n",
    "        silver_b_df = dlt.read(silver_b_table_name)\n",
    "        logger.info(f\"Successfully read Silver B table: {silver_b_table_name}\")\n",
    "        \n",
    "        # Get row count for logging\n",
    "        row_count = silver_b_df.count()\n",
    "        logger.info(f\"Silver B table contains {row_count} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read Silver B table {silver_b_table_name}: {e}\")\n",
    "        # Return empty DataFrame if Silver B doesn't exist\n",
    "        empty_schema = StructType([StructField(\"placeholder\", StringType(), True)])\n",
    "        return spark.createDataFrame([], empty_schema)\n",
    "    \n",
    "    # Start Silver C processing\n",
    "    result_df = silver_b_df\n",
    "    \n",
    "    # Step 1: Apply deduplication\n",
    "    dedup_keys = get_deduplication_keys(domain)\n",
    "    result_df = apply_deduplication(result_df, dedup_keys)\n",
    "    \n",
    "    # Step 2: Add quality score (placeholder for future enhancement)\n",
    "    result_df = add_quality_score(result_df)\n",
    "    \n",
    "    # Step 3: Apply quality filtering (placeholder for future enhancement)\n",
    "    min_quality_score = QUALITY_THRESHOLDS.get(\"min_quality_score\", 0.7)\n",
    "    # Commented out for now - enable when quality scoring is fully implemented\n",
    "    # result_df = apply_quality_filtering(result_df, min_quality_score)\n",
    "    \n",
    "    # Step 4: Add processing metadata\n",
    "    result_df = result_df.withColumn(\"processed_timestamp\", F.current_timestamp())\n",
    "    result_df = result_df.withColumn(\"pipeline_stage\", F.lit(\"silver_c\"))\n",
    "    \n",
    "    final_count = result_df.count()\n",
    "    logger.info(f\"Silver C processing complete - {final_count} rows, columns: {result_df.columns}\")\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Expectations for Silver C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_silver_c_quality_checks(df: DataFrame, source: str, lob: str, domain: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply data quality checks specific to Silver C stage\n",
    "    \n",
    "    Args:\n",
    "        df: Silver C DataFrame\n",
    "        source: Source identifier\n",
    "        lob: Line of business\n",
    "        domain: Domain\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with quality checks applied\n",
    "    \"\"\"\n",
    "    if not config.validation_enabled:\n",
    "        logger.info(\"Data quality validation is disabled\")\n",
    "        return df\n",
    "    \n",
    "    logger.info(f\"Applying Silver C quality checks for {source}/{lob}/{domain}\")\n",
    "    \n",
    "    # Apply standard required field validation\n",
    "    validated_df = stages.validate_required_fields(df, config, source, lob, domain)\n",
    "    \n",
    "    # Additional Silver C specific validations\n",
    "    \n",
    "    # Check for duplicate records (should be minimal after deduplication)\n",
    "    dedup_keys = get_deduplication_keys(domain)\n",
    "    available_keys = [key for key in dedup_keys if key in validated_df.columns]\n",
    "    \n",
    "    if available_keys:\n",
    "        duplicate_count = (\n",
    "            validated_df.groupBy(*available_keys)\n",
    "                       .count()\n",
    "                       .filter(F.col(\"count\") > 1)\n",
    "                       .count()\n",
    "        )\n",
    "        \n",
    "        if duplicate_count > 0:\n",
    "            logger.warning(f\"Found {duplicate_count} duplicate groups after deduplication for {source}/{lob}/{domain}\")\n",
    "        else:\n",
    "            logger.info(f\"No duplicates found after deduplication for {source}/{lob}/{domain}\")\n",
    "    \n",
    "    # Check quality score distribution if present\n",
    "    if \"quality_score\" in validated_df.columns:\n",
    "        quality_stats = validated_df.select(\n",
    "            F.avg(\"quality_score\").alias(\"avg_quality\"),\n",
    "            F.min(\"quality_score\").alias(\"min_quality\"),\n",
    "            F.max(\"quality_score\").alias(\"max_quality\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        logger.info(f\"Quality score statistics for {source}/{lob}/{domain}: \"\n",
    "                   f\"avg={quality_stats['avg_quality']:.3f}, \"\n",
    "                   f\"min={quality_stats['min_quality']:.3f}, \"\n",
    "                   f\"max={quality_stats['max_quality']:.3f}\")\n",
    "    \n",
    "    logger.info(f\"Quality checks complete - {validated_df.count()} rows passed validation\")\n",
    "    return validated_df\n",
    "\n",
    "\n",
    "def validate_silver_c_completeness(df: DataFrame, source: str, lob: str, domain: str) -> bool:\n",
    "    \"\"\"\n",
    "    Validate that Silver C processing was completed successfully\n",
    "    \n",
    "    Args:\n",
    "        df: Processed Silver C DataFrame\n",
    "        source: Source identifier\n",
    "        lob: Line of business\n",
    "        domain: Domain\n",
    "        \n",
    "    Returns:\n",
    "        Boolean indicating if processing was successful\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if DataFrame has data\n",
    "        row_count = df.count()\n",
    "        if row_count == 0:\n",
    "            logger.warning(f\"No data in Silver C table for {source}/{lob}/{domain}\")\n",
    "            return False\n",
    "        \n",
    "        # Check if required metadata columns are present\n",
    "        required_metadata = [\"processed_timestamp\", \"pipeline_stage\"]\n",
    "        missing_metadata = [col for col in required_metadata if col not in df.columns]\n",
    "        \n",
    "        if missing_metadata:\n",
    "            logger.warning(f\"Missing metadata columns in Silver C for {source}/{lob}/{domain}: {missing_metadata}\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"Silver C completeness validation passed for {source}/{lob}/{domain} - {row_count} rows\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Silver C completeness validation failed for {source}/{lob}/{domain}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLT Table Definitions\n",
    "\n",
    "Dynamic generation of DLT tables for each source/lob/domain combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate DLT table definitions for each source/lob/domain combination\n",
    "for source, lob, domain in source_combinations:\n",
    "    \n",
    "    # Create a closure to capture the current values of source, lob, domain\n",
    "    def make_silver_c_table(src, lb, dom):\n",
    "        \n",
    "        @dlt.table(\n",
    "            name=f\"{src}_{lb}_{dom}_silver_c\",\n",
    "            comment=f\"Silver C stage for {src}/{lb}/{dom} - Final Silver layer with deduplication and quality filtering\",\n",
    "            table_properties={\n",
    "                \"quality\": \"silver\",\n",
    "                \"layer\": \"silver_c\",\n",
    "                \"source\": src,\n",
    "                \"lob\": lb,\n",
    "                \"domain\": dom,\n",
    "                \"deduplication_keys\": \",\".join(get_deduplication_keys(dom))\n",
    "            }\n",
    "        )\n",
    "        @dlt.expect_all_or_drop(\"valid_silver_c_data\")\n",
    "        def silver_c_table():\n",
    "            \"\"\"\n",
    "            Create Silver C table with deduplication and filtering from Silver B\n",
    "            \"\"\"\n",
    "            # Create Silver C DataFrame\n",
    "            silver_c_df = create_silver_c_table(src, lb, dom)\n",
    "            \n",
    "            # Validate processing completeness\n",
    "            if not validate_silver_c_completeness(silver_c_df, src, lb, dom):\n",
    "                logger.warning(f\"Silver C completeness validation failed for {src}/{lb}/{dom}\")\n",
    "            \n",
    "            # Apply quality checks\n",
    "            validated_df = apply_silver_c_quality_checks(silver_c_df, src, lb, dom)\n",
    "            \n",
    "            return validated_df\n",
    "        \n",
    "        return silver_c_table\n",
    "    \n",
    "    # Create the table function and add it to the global namespace\n",
    "    table_func = make_silver_c_table(source, lob, domain)\n",
    "    globals()[f\"{source}_{lob}_{domain}_silver_c\"] = table_func\n",
    "    \n",
    "    dedup_keys = get_deduplication_keys(domain)\n",
    "    print(f\"Created DLT table definition: {source}_{lob}_{domain}_silver_c (dedup keys: {dedup_keys})\")\n",
    "\n",
    "print(f\"\\nTotal Silver C tables defined: {len(source_combinations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced DLT Expectations for Final Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced DLT expectations for Silver C final data quality\n",
    "# These ensure the final Silver layer meets high-quality standards\n",
    "\n",
    "def create_final_quality_expectations():\n",
    "    \"\"\"\n",
    "    Create advanced DLT expectations for Silver C final quality validation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Expect no duplicate records based on business keys\n",
    "    @dlt.expect(\"no_duplicates_on_business_keys\")\n",
    "    def expect_no_duplicates(df):\n",
    "        # This would be customized per domain\n",
    "        return True  # Placeholder - actual implementation would check for duplicates\n",
    "    \n",
    "    # Expect reasonable quality scores\n",
    "    @dlt.expect(\"acceptable_quality_scores\")\n",
    "    def expect_quality_scores(df):\n",
    "        if \"quality_score\" in df.columns:\n",
    "            return F.col(\"quality_score\") >= 0.5\n",
    "        return True\n",
    "    \n",
    "    # Expect processed timestamp to be recent\n",
    "    @dlt.expect(\"recent_processing_timestamp\")\n",
    "    def expect_recent_timestamp(df):\n",
    "        if \"processed_timestamp\" in df.columns:\n",
    "            # Expect processing within last 24 hours\n",
    "            return F.col(\"processed_timestamp\") >= F.date_sub(F.current_timestamp(), 1)\n",
    "        return True\n",
    "    \n",
    "    # Expect pipeline stage to be correctly set\n",
    "    @dlt.expect(\"correct_pipeline_stage\")\n",
    "    def expect_pipeline_stage(df):\n",
    "        if \"pipeline_stage\" in df.columns:\n",
    "            return F.col(\"pipeline_stage\") == \"silver_c\"\n",
    "        return True\n",
    "    \n",
    "    logger.info(\"Advanced final quality expectations defined\")\n",
    "\n",
    "# Uncomment the line below to enable advanced final quality expectations\n",
    "# create_final_quality_expectations()\n",
    "\n",
    "print(\"Advanced final quality expectations available but not enabled by default\")\n",
    "print(\"Uncomment create_final_quality_expectations() call to enable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Monitoring and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log pipeline configuration for monitoring\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"SILVER C DLT PIPELINE CONFIGURATION\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Pipeline stage: Silver C (Deduplication & Filtering)\")\n",
    "logger.info(f\"Silver schema: {config.silver_schema}\")\n",
    "logger.info(f\"Validation enabled: {config.validation_enabled}\")\n",
    "logger.info(f\"Source combinations: {len(source_combinations)}\")\n",
    "logger.info(f\"Quality thresholds: {QUALITY_THRESHOLDS}\")\n",
    "\n",
    "logger.info(\"\\nDeduplication configuration:\")\n",
    "for domain, keys in DEDUPLICATION_CONFIG.items():\n",
    "    logger.info(f\"  {domain}: {keys}\")\n",
    "\n",
    "logger.info(\"\\nTable dependencies:\")\n",
    "for i, (source, lob, domain) in enumerate(source_combinations, 1):\n",
    "    dedup_keys = get_deduplication_keys(domain)\n",
    "    logger.info(f\"  {i}. {source}/{lob}/{domain}:\")\n",
    "    logger.info(f\"     Input:  {source}_{lob}_{domain}_silver_b\")\n",
    "    logger.info(f\"     Output: {source}_{lob}_{domain}_silver_c\")\n",
    "    logger.info(f\"     Dedup:  {dedup_keys}\")\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"SILVER C DLT PIPELINE READY\")\n",
    "logger.info(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "To use this notebook in Databricks DLT:\n",
    "\n",
    "1. **Create a new DLT Pipeline** in the Databricks workspace\n",
    "2. **Set the source** to this notebook (`silver_c_dlt.ipynb`)\n",
    "3. **Configure pipeline settings**:\n",
    "   - Target schema: `silver` (or your configured silver schema)\n",
    "   - Pipeline mode: `Triggered` for batch processing\n",
    "   - Cluster configuration: Based on your data volume\n",
    "4. **Ensure dependencies**:\n",
    "   - Silver B tables must exist (from Silver B DLT pipeline)\n",
    "   - Utils modules must be available in the workspace\n",
    "5. **Pipeline Dependencies**:\n",
    "   - This pipeline should run AFTER the Silver B pipeline\n",
    "   - Set up proper scheduling or triggering to ensure Silver B tables are available\n",
    "\n",
    "**Input Tables**: This pipeline reads from:\n",
    "- `{silver_schema}.{source}_{lob}_{domain}_silver_b`\n",
    "\n",
    "**Output Tables**: This pipeline creates:\n",
    "- `{silver_schema}.{source}_{lob}_{domain}_silver_c`\n",
    "\n",
    "**Dependencies**: This pipeline depends on:\n",
    "- Silver B tables: `{silver_schema}.{source}_{lob}_{domain}_silver_b`\n",
    "\n",
    "**Processing Logic**:\n",
    "1. **Deduplication**: Removes duplicate records based on domain-specific business keys\n",
    "2. **Quality Scoring**: Adds quality scores based on data completeness (placeholder)\n",
    "3. **Quality Filtering**: Filters out low-quality records (placeholder, currently disabled)\n",
    "4. **Metadata Addition**: Adds processing timestamp and pipeline stage information\n",
    "\n",
    "**Domain-Specific Deduplication Keys**:\n",
    "- **Pharmacy**: patient_id, ndc, service_date\n",
    "- **Medical**: patient_id, procedure_code, service_date\n",
    "- **Member**: member_id, effective_date\n",
    "- **Default**: patient_id, service_date\n",
    "\n",
    "**Future Enhancements**:\n",
    "- Advanced quality scoring algorithms\n",
    "- Configurable deduplication rules through business rules tables\n",
    "- Data lineage tracking\n",
    "- Performance optimization for large datasets\n",
    "\n",
    "**Monitoring**:\n",
    "- Check DLT pipeline logs for deduplication statistics\n",
    "- Monitor quality score distributions\n",
    "- Validate that duplicate counts are minimal after processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}